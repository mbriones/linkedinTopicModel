t_linkedinData$name <- gsub(";.*","",t_linkedinData$name)
linkedinData <- multmerge("/Users/miguelbriones/ByteflowProjects/LinkedInScrape/linkedindata")
library(stringr)
library(tidyr)
library(data.table)
library(dplyr)
readFun <- function( filename ) {
#load data
data <- tryCatch(read.csv(filename, stringsAsFactors = F, na.strings=c(""," ","NA")), error=function(e) NULL)
#remove X column
data[c("X")] <- list(NULL)
data <- if ("endorsements" %in% colnames(data)) {
#create new column to combine number of endorsements and skill
unite(data, endor, c(endorsements, name), remove=FALSE)
#remove name column and endorsement column
#data[ ,!(colnames(data) == "name")]
#data[ ,!(colnames(data) == "endorsements")]
} else {
#just return data
data
}
#remove columns
#data <- if ("name" %in% colnames(data)) {
#remove name column and endorsement column
#         data[ ,!(colnames(data) == "name")]
#prac[c("endorsements")] <- list(NULL)
#      } else {
#       data
#    }
data <- if ("endorsements" %in% colnames(data)) {
data[ ,!(colnames(data) == "endorsements")]
} else {
data
}
#new column name
data <- if ("X0" %in% colnames(data)) {
rename(data, "companiesFollowing" = "X0")
} else {
data
}
#character in column 4
data$followers <- as.character(data$followers)
#have every row have a column name
data <- as.data.frame(unlist(data))
#move rownames into first column
data <- tibble::rownames_to_column(data, "Name")
#remove NA values
data <- na.omit(data)
data$`unlist(data)` <- as.character(data$`unlist(data)`)
#remove NA_NA string
data <- as.data.frame(data[!grepl("NA_NA", data$`unlist(data)`),])
#single out name by itself
data$`unlist(data)` <- gsub("NA_", "",data$`unlist(data)`)
#change first column to name
data$Name[data$Name == "endor1"] <- "name"
data$Name <- gsub("[0-9]+", "",data$Name)
colnames(data) <- c("Title", "Data")
#collapse each column by name
data <- data %>%
group_by(Title) %>%
summarise_each(funs(paste(., collapse = "; ")))
return(data)
}
multmerge = function(mypath){
filenames=list.files(path=mypath, full.names=TRUE)
datalist = lapply(filenames, readFun)
Reduce(function(x,y) {merge(x,y, by = "Title", all = TRUE)}, datalist)
}
linkedinData <- multmerge("/Users/miguelbriones/ByteflowProjects/LinkedInScrape/linkedindata")
linkedinData <- multmerge("/Users/miguelbriones/ByteflowProjects/LinkedInScrape/linkedindata")
linkedinData <- multmerge("/Users/miguelbriones/ByteflowProjects/LinkedInScrape/linkedindata")
readFun <- function( filename ) {
#load data
data <- tryCatch(read.csv(filename, stringsAsFactors = F, na.strings=c(""," ","NA")), error=function(e) NULL)
#remove X column
data[c("X")] <- list(NULL)
data <- if ("endorsements" %in% colnames(data)) {
#create new column to combine number of endorsements and skill
unite(data, endor, c(endorsements, name), remove=FALSE)
#remove name column and endorsement column
#data[ ,!(colnames(data) == "name")]
#data[ ,!(colnames(data) == "endorsements")]
} else {
#just return data
data
}
#remove columns
#data <- if ("name" %in% colnames(data)) {
#remove name column and endorsement column
#         data[ ,!(colnames(data) == "name")]
#prac[c("endorsements")] <- list(NULL)
#      } else {
#       data
#    }
data <- if ("endorsements" %in% colnames(data)) {
data[ ,!(colnames(data) == "endorsements")]
} else {
data
}
#new column name
data <- if ("X0" %in% colnames(data)) {
dplyr::rename(data, "companiesFollowing" = "X0")
} else {
data
}
#character in column 4
data$followers <- as.character(data$followers)
#have every row have a column name
data <- as.data.frame(unlist(data))
#move rownames into first column
data <- tibble::rownames_to_column(data, "Name")
#remove NA values
data <- na.omit(data)
data$`unlist(data)` <- as.character(data$`unlist(data)`)
#remove NA_NA string
data <- as.data.frame(data[!grepl("NA_NA", data$`unlist(data)`),])
#single out name by itself
data$`unlist(data)` <- gsub("NA_", "",data$`unlist(data)`)
#change first column to name
data$Name[data$Name == "endor1"] <- "name"
data$Name <- gsub("[0-9]+", "",data$Name)
colnames(data) <- c("Title", "Data")
#collapse each column by name
data <- data %>%
group_by(Title) %>%
summarise_each(funs(paste(., collapse = "; ")))
return(data)
}
# execute a function that executes readFun across all files, outputting a data frame
multmerge = function(mypath){
filenames=list.files(path=mypath, full.names=TRUE)
datalist = lapply(filenames, readFun)
Reduce(function(x,y) {merge(x,y, by = "Title", all = TRUE)}, datalist)
}
linkedinData <- multmerge("/Users/miguelbriones/ByteflowProjects/LinkedInScrape/linkedindata")
t_linkedinData <- transpose(linkedinData)
# change the column names to the first row and remove the first row
colnames(t_linkedinData) = t_linkedinData[1, ]
t_linkedinData = t_linkedinData[-1,]
#move name column to the left
col_idx <- grep("name", names(t_linkedinData))
t_linkedinData <- t_linkedinData[, c(col_idx, (1:ncol(t_linkedinData))[-col_idx])]
#remove NA rows from the name column
t_linkedinData <-t_linkedinData[!(is.na(t_linkedinData$name)),]
#isolate name data
t_linkedinData$name <- gsub(";.*","",t_linkedinData$name)
View(t_linkedinData)
write.csv(t_linkedinData, "linkedindata.csv")
setwd("~/Desktop/Survey")
shiny::runApp()
runApp('~/ByteflowProjects/InsuranceShiny/InsuranceSample')
runApp('~/ByteflowProjects/QueensLinkedIn/CUNYJobPrediction')
runApp('~/ByteflowProjects/QueensLinkedIn/UniJobPrediction')
runApp('~/ByteflowProjects/QueensLinkedIn/CUNYJobPrediction')
runApp('Desktop/LPS/LPSApp.R')
runApp('Desktop/LPS/LPSApp.R')
install.packages("shinyjs")
runApp('Desktop/LPS/LPSApp.R')
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(caret)
library(ggplot2)
data <- read.csv("Table.csv", stringsAsFactors = FALSE)
names(data)[names(data) == "DAÑO"] <- "DANO"
data <- data.frame(lapply(data, function(x) {
gsub("ñ", "n", x)
}))
data <- data.frame(lapply(data, function(x) {
gsub("ó", "o", x)
}))
data <- data.frame(lapply(data, function(x) {
gsub("á", "a", x)
}))
data <- data.frame(lapply(data, function(x) {
gsub("é", "e", x)
}))
data <- data.frame(lapply(data, function(x) {
gsub("Í", "I", x)
}))
data <- data.frame(lapply(data, function(x) {
gsub("Ñ", "N", x)
}))
data <- data.frame(lapply(data, as.character), stringsAsFactors=FALSE)
head(data)
et.seed(101)
set.seed(101)
alpha     <- 0.7 # percentage of training set
datfac <- datex
---
title: "AON Data Exploration"
author: "Byteflow Dynamics"
date: "11/20/2017"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(rpart)
library(rpart.plot)
library(caret)
library(ggplot2)
```
#Data
```{r}
data <- read.csv("Table.csv", stringsAsFactors = FALSE)
names(data)[names(data) == "DAÑO"] <- "DANO"
data <- data.frame(lapply(data, function(x) {
gsub("ñ", "n", x)
}))
data <- data.frame(lapply(data, function(x) {
gsub("ó", "o", x)
}))
data <- data.frame(lapply(data, function(x) {
gsub("á", "a", x)
}))
data <- data.frame(lapply(data, function(x) {
gsub("é", "e", x)
}))
data <- data.frame(lapply(data, function(x) {
gsub("Í", "I", x)
}))
data <- data.frame(lapply(data, function(x) {
gsub("Ñ", "N", x)
}))
data <- data.frame(lapply(data, as.character), stringsAsFactors=FALSE)
head(data)
```
#Data Wrangling
We removed rows where IMPORTE is missing, as well as rows where DAÑO is "No Definido" or "Desconocido" and COMUNIDAD_AUTONOMA is missing. We also kept only the categorical, non-coded variables (plus IMPORTE) for the purpose of this analysis.
```{r}
dat <- data %>%
select(-1) %>%
filter(!is.na(IMPORTE)) %>% #Remove rows where IMPORTE is NA
filter(COMUNIDAD_AUTONOMA != "") %>%
select(c(DANO,ESPECIALIDAD,LUGAR,CENTRO,IMPORTE,COMUNIDAD_AUTONOMA))
dat <- dat[!(dat$DANO == "No Definido" | dat$DANO == "Desconocido"), ]
dat$IMPORTE <- as.numeric(dat$IMPORTE)
dat <- dat[dat$IMPORTE >= 0, ]
head(dat)
```
#Analysis of Variance
We performed analysis of variance on the dataset to determine which variables are highly correlated with IMPORTE.
```{r}
aovres <- aov(IMPORTE ~ DANO + ESPECIALIDAD + LUGAR + COMUNIDAD_AUTONOMA, dat)
summary(aovres)
```
The result shows variables DAÑO,ESPECIALIDAD,LUGAR,  COMUNIDAD_AUTONOMA are all significant.
# Exploratory data analysis
Here we show some graphs related to the two variables DAÑO and COMUNIDAD_AUTONOMA. The frequency plot shows which types of injury or regions are more frequently reported. The mean IMPORTE plot shows the average cost of a claim for each type of injury or from each region.
- DAÑO
```{r}
dat_dano <- dat %>%
group_by(DANO) %>%
summarise(count = n(), cost_mean = mean(IMPORTE)) %>%
arrange(desc(cost_mean))
ggplot(dat_dano) +
geom_col(aes(x=DANO, y=count)) +
coord_flip() +
ggtitle("Frequency of each DAÑO value")
ggplot(dat_dano) +
geom_col(aes(x=DANO, y=cost_mean)) +
coord_flip() +
ggtitle("Mean IMPORTE for each DAÑO value")
```
From these plots, it looks like Osteomuscular, Muerte, and Daño Material are the most frequent types of injury, and Neurológico grave, Daño al niño o al recién nacid, Coma are the most expensive types of injury. It is understandable that these severe injuries are more expensive and less frequent.
- COMUNIDAD_AUTONOMA
```{r}
dat_ca <- dat %>%
group_by(COMUNIDAD_AUTONOMA) %>%
summarise(count = n(), cost_mean = mean(IMPORTE)) %>%
arrange(desc(cost_mean))
ggplot(dat_ca) +
geom_col(aes(x=COMUNIDAD_AUTONOMA, y=count)) +
coord_flip() +
ggtitle("Frequency of each COMUNIDAD_AUTONOMA value")
ggplot(dat_ca) +
geom_col(aes(x=COMUNIDAD_AUTONOMA, y=cost_mean)) +
coord_flip() +
ggtitle("Mean IMPORTE for each COMUNIDAD_AUTONOMA value")
```
The average claim cost varies from region to region.
# Basic Machine Learning
We use Classification Trees, which is a machine learning method used to predict categorical outcomes, to model and predict the cost level (high, medium, low) based on the injury type (DAÑO) and region (COMUNIDAD_AUTONOMA). First, I divided IMPORTE into 2 cost levels.
```{r}
# new section
dat$log <- log10(dat$IMPORTE)
dat_min <- dat %>%
filter(dat$IMPORTE <= 3) %>%
mutate(cost = 'Bajo')
dat_hi <- dat %>%
filter(dat$IMPORTE >3)%>%
mutate(cost = 'Alto')
datex <- bind_rows(dat_min, dat_hi)
head(datex)
```
Next, we build a classification tree model. We train the model using 70% of the data and check the fit.
```{r}
set.seed(101)
alpha     <- 0.7 # percentage of training set
datfac <- datex
datfac$DANO <- factor(datfac$DAN)
datfac$COMUNIDAD_AUTONOMA <- factor(datfac$COMUNIDAD_AUTONOMA)
datfac$LUGAR <- factor(datfac$LUGAR)
datfac$ESPECIALIDAD <- factor(datfac$ESPECIALIDAD)
datfac$cost <- factor(datfac$cost)
inTrain   <- sample(1:nrow(datfac), alpha * nrow(datfac))
train.set <- datfac[inTrain,]
test.set  <- datfac[-inTrain,]
fit <- rpart(cost ~ DANO + COMUNIDAD_AUTONOMA,
method="class", data=train.set) #it will take a few min
printcp(fit)
set.seed(101)
alpha     <- 0.7 # percentage of training set
datfac <- datex
datfac$DANO <- factor(datfac$DAN)
datfac$COMUNIDAD_AUTONOMA <- factor(datfac$COMUNIDAD_AUTONOMA)
datfac$LUGAR <- factor(datfac$LUGAR)
datfac$ESPECIALIDAD <- factor(datfac$ESPECIALIDAD)
datfac$cost <- factor(datfac$cost)
inTrain   <- sample(1:nrow(datfac), alpha * nrow(datfac))
train.set <- datfac[inTrain,]
test.set  <- datfac[-inTrain,]
fit <- rpart(cost ~ DANO + COMUNIDAD_AUTONOMA,
method="class", data=train.set) #it will take a few min
printcp(fit)
rpart.plot(fit)
pred <- predict(fit,test.set)
pred <- predict(fit,test.set)
idx <- apply(pred, c(1), maxidx)
```{r}
maxidx <- function(arr) {
return(which(arr == max(arr)))
}
pred <- predict(fit,test.set)
idx <- apply(pred, c(1), maxidx)
prediction <- c('high', 'low')[idx]
confMat <- table(test.set$cost, prediction)
accuracy <- sum(diag(confMat))/sum(confMat)
accuracy
confMat
shiny::runApp('ByteflowProjects/InsuranceShiny/InsuranceBeta')
shiny::runApp('ByteflowProjects/InsuranceShiny/InsuranceSample')
shiny::runApp('ByteflowProjects/InsuranceShiny/InsuranceSample')
readFun <- function( filename ) {
#load data
data <- tryCatch(read.csv(filename, stringsAsFactors = F, na.strings=c(""," ","NA")), error=function(e) NULL)
#remove X column
data[c("X")] <- list(NULL)
data <- if ("endorsements" %in% colnames(data)) {
#create new column to combine number of endorsements and skill
unite(data, endor, c(endorsements, name), remove=FALSE)
#remove name column and endorsement column
#data[ ,!(colnames(data) == "name")]
#data[ ,!(colnames(data) == "endorsements")]
} else {
#just return data
data
}
#remove columns
#data <- if ("name" %in% colnames(data)) {
#remove name column and endorsement column
#         data[ ,!(colnames(data) == "name")]
#prac[c("endorsements")] <- list(NULL)
#      } else {
#       data
#    }
data <- if ("endorsements" %in% colnames(data)) {
data[ ,!(colnames(data) == "endorsements")]
} else {
data
}
#new column name
data <- if ("X0" %in% colnames(data)) {
dplyr::rename(data, "companiesFollowing" = "X0")
} else {
data
}
#character in column 4
data$followers <- as.character(data$followers)
#have every row have a column name
data <- as.data.frame(unlist(data))
#move rownames into first column
data <- tibble::rownames_to_column(data, "Name")
#remove NA values
data <- na.omit(data)
data$`unlist(data)` <- as.character(data$`unlist(data)`)
#remove NA_NA string
data <- as.data.frame(data[!grepl("NA_NA", data$`unlist(data)`),])
#single out name by itself
data$`unlist(data)` <- gsub("NA_", "",data$`unlist(data)`)
#change first column to name
data$Name[data$Name == "endor1"] <- "name"
data$Name <- gsub("[0-9]+", "",data$Name)
colnames(data) <- c("Title", "Data")
#collapse each column by name
data <- data %>%
group_by(Title) %>%
summarise_each(funs(paste(., collapse = "; ")))
return(data)
}
# execute a function that executes readFun across all files, outputting a data frame
multmerge = function(mypath){
filenames=list.files(path=mypath, full.names=TRUE)
datalist = lapply(filenames, readFun)
Reduce(function(x,y) {merge(x,y, by = "Title", all = TRUE)}, datalist)
}
#create a dataframe by using above functions to merge all data
linkedinData <- multmerge("/Users/miguelbriones/Desktop/Insight/LinkedinData/DataScience")
#transpose the dataframe
t_linkedinData <- transpose(linkedinData)
# change the column names to the first row and remove the first row
colnames(t_linkedinData) = t_linkedinData[1, ]
t_linkedinData = t_linkedinData[-1,]
#move name column to the left
col_idx <- grep("name", names(t_linkedinData))
t_linkedinData <- t_linkedinData[, c(col_idx, (1:ncol(t_linkedinData))[-col_idx])]
#remove NA rows from the name column
t_linkedinData <-t_linkedinData[!(is.na(t_linkedinData$name)),]
#isolate name data
t_linkedinData$name <- gsub(";.*","",t_linkedinData$name)
setwd("~/Desktop/Insight/LinkedinData")
linkedinData <- multmerge("/Users/miguelbriones/Desktop/Insight/LinkedinData/DataScience")
library(stringr)
library(tidyr)
library(data.table)
library(dplyr)
library(plyr)
#create a dataframe by using above functions to merge all data
linkedinData <- multmerge("/Users/miguelbriones/Desktop/Insight/LinkedinData/DataScience")
#transpose the dataframe
t_linkedinData <- transpose(linkedinData)
# change the column names to the first row and remove the first row
colnames(t_linkedinData) = t_linkedinData[1, ]
t_linkedinData = t_linkedinData[-1,]
#move name column to the left
col_idx <- grep("name", names(t_linkedinData))
t_linkedinData <- t_linkedinData[, c(col_idx, (1:ncol(t_linkedinData))[-col_idx])]
#remove NA rows from the name column
t_linkedinData <-t_linkedinData[!(is.na(t_linkedinData$name)),]
#isolate name data
t_linkedinData$name <- gsub(";.*","",t_linkedinData$name)
dataScienceuno <- t_linkedinData
View(dataScienceuno)
linkedinData <- multmerge("/Users/miguelbriones/Desktop/Insight/LinkedinData/DataScienceDos")
#transpose the dataframe
t_linkedinData <- transpose(linkedinData)
# change the column names to the first row and remove the first row
colnames(t_linkedinData) = t_linkedinData[1, ]
t_linkedinData = t_linkedinData[-1,]
#move name column to the left
col_idx <- grep("name", names(t_linkedinData))
t_linkedinData <- t_linkedinData[, c(col_idx, (1:ncol(t_linkedinData))[-col_idx])]
#remove NA rows from the name column
t_linkedinData <-t_linkedinData[!(is.na(t_linkedinData$name)),]
#isolate name data
t_linkedinData$name <- gsub(";.*","",t_linkedinData$name)
dataSciencedos <- t_linkedinData
View(dataSciencedos)
linkedinData <- multmerge("/Users/miguelbriones/Desktop/Insight/LinkedinData/DataScienceDosTwo")
#transpose the dataframe
t_linkedinData <- transpose(linkedinData)
# change the column names to the first row and remove the first row
colnames(t_linkedinData) = t_linkedinData[1, ]
t_linkedinData = t_linkedinData[-1,]
#move name column to the left
col_idx <- grep("name", names(t_linkedinData))
t_linkedinData <- t_linkedinData[, c(col_idx, (1:ncol(t_linkedinData))[-col_idx])]
#remove NA rows from the name column
t_linkedinData <-t_linkedinData[!(is.na(t_linkedinData$name)),]
#isolate name data
t_linkedinData$name <- gsub(";.*","",t_linkedinData$name)
dataSciencedostwo <- t_linkedinData
linkedinData <- multmerge("/Users/miguelbriones/Desktop/Insight/LinkedinData/DataScienceDosThree")
#transpose the dataframe
t_linkedinData <- transpose(linkedinData)
# change the column names to the first row and remove the first row
colnames(t_linkedinData) = t_linkedinData[1, ]
t_linkedinData = t_linkedinData[-1,]
#move name column to the left
col_idx <- grep("name", names(t_linkedinData))
t_linkedinData <- t_linkedinData[, c(col_idx, (1:ncol(t_linkedinData))[-col_idx])]
#remove NA rows from the name column
t_linkedinData <-t_linkedinData[!(is.na(t_linkedinData$name)),]
#isolate name data
t_linkedinData$name <- gsub(";.*","",t_linkedinData$name)
dataSciencedosthree <- t_linkedinData
#create a dataframe by using above functions to merge all data
linkedinData <- multmerge("/Users/miguelbriones/Desktop/Insight/LinkedinData/DataScienceTres")
#transpose the dataframe
t_linkedinData <- transpose(linkedinData)
# change the column names to the first row and remove the first row
colnames(t_linkedinData) = t_linkedinData[1, ]
t_linkedinData = t_linkedinData[-1,]
#move name column to the left
col_idx <- grep("name", names(t_linkedinData))
t_linkedinData <- t_linkedinData[, c(col_idx, (1:ncol(t_linkedinData))[-col_idx])]
#remove NA rows from the name column
t_linkedinData <-t_linkedinData[!(is.na(t_linkedinData$name)),]
#isolate name data
t_linkedinData$name <- gsub(";.*","",t_linkedinData$name)
dataSciencetres <- t_linkedinData
View(dataSciencedos)
View(dataSciencedosthree)
View(dataSciencetres)
View(dataSciencedostwo)
datascience <- rbind(dataScienceuno, dataSciencedos, dataSciencedostwo, dataSciencedosthree, dataSciencetres)
View(datascience)
setwd("~/Desktop/Insight/LinkedinData/Datasets")
write.csv(datasience, "datascientist.csv")
write.csv(datascience, "datascientist.csv")
